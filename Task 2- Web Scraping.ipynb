{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# --- SECTION 1: INSTALLATION AND SETUP FOR GOOGLE COLAB ---\n",
        "print(\"Installing Python packages...\")\n",
        "!pip install selenium webdriver-manager beautifulsoup4 > /dev/null\n",
        "print(\"Python packages installed.\")\n",
        "\n",
        "print(\"Setting up Google Chrome stable browser...\")\n",
        "!apt-get update > /dev/null\n",
        "!wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | apt-key add - > /dev/null\n",
        "!echo \"deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main\" > /etc/apt/sources.list.d/google-chrome.list\n",
        "!apt-get update > /dev/null\n",
        "!apt-get install google-chrome-stable -y > /dev/null\n",
        "print(\"Google Chrome stable browser installed.\")\n",
        "\n",
        "# --- SECTION 2: MAIN CODE ---\n",
        "import re\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import csv\n",
        "\n",
        "# Configure Chrome options for headless execution\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "chrome_options.add_argument('--start-maximized')\n",
        "chrome_options.binary_location = '/usr/bin/google-chrome-stable'\n",
        "\n",
        "# Initialize ChromeDriver service\n",
        "try:\n",
        "    service = Service(ChromeDriverManager().install())\n",
        "except Exception as e:\n",
        "    raise SystemExit(f\"Error initializing ChromeDriver service: {e}\")\n",
        "\n",
        "def get_webdriver():\n",
        "    return webdriver.Chrome(service=service, options=chrome_options)\n",
        "\n",
        "# Web scraping logic\n",
        "base_url = \"https://trt.global/world/politics\"\n",
        "all_articles_data = []\n",
        "scraped_urls = set()\n",
        "driver = None\n",
        "\n",
        "try:\n",
        "    driver = get_webdriver()\n",
        "    driver.get(base_url)\n",
        "    time.sleep(3)\n",
        "\n",
        "    # Handle Cookie Consent Pop-up\n",
        "    try:\n",
        "        cookie_accept_button = WebDriverWait(driver, 5).until(\n",
        "            EC.element_to_be_clickable((By.XPATH, \"//button[contains(translate(text(), 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', 'abcdefghijklmnopqrstuvwxyz'), 'accept')]\"))\n",
        "        )\n",
        "        cookie_accept_button.click()\n",
        "        time.sleep(2)\n",
        "    except TimeoutException:\n",
        "        pass\n",
        "\n",
        "    # Dynamic Scrolling to Load Articles\n",
        "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "    scroll_pause_time = 3\n",
        "    max_scroll_attempts = 20\n",
        "    scroll_attempts = 0\n",
        "\n",
        "    while scroll_attempts < max_scroll_attempts:\n",
        "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "        time.sleep(scroll_pause_time)\n",
        "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "        scroll_attempts += 1\n",
        "        if new_height == last_height:\n",
        "            break\n",
        "        last_height = new_height\n",
        "\n",
        "    # Handle Pagination (if available)\n",
        "    while True:\n",
        "        html_content = driver.page_source\n",
        "        soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "        # Extract Article Links and Data\n",
        "        article_selectors = ['article', 'div[class*=\"news-item\"], div[class*=\"article\"]', 'li[class*=\"item\"]']\n",
        "        article_links_to_visit = []\n",
        "        for selector in article_selectors:\n",
        "            articles = soup.select(selector)\n",
        "            for article in articles:\n",
        "                link_tag = article.find('a', href=True)\n",
        "                if link_tag and link_tag['href']:\n",
        "                    article_url = link_tag['href']\n",
        "                    # Ensure full URL and handle redirects\n",
        "                    if not article_url.startswith('http'):\n",
        "                        article_url = 'https://trt.global' + article_url if article_url.startswith('/') else base_url.rstrip('/') + '/' + article_url.lstrip('/')\n",
        "                    try:\n",
        "                        driver.get(article_url)\n",
        "                        time.sleep(1)\n",
        "                        article_url = driver.current_url  # Get the resolved URL after redirect\n",
        "                    except WebDriverException:\n",
        "                        continue\n",
        "                    if article_url in scraped_urls:\n",
        "                        continue\n",
        "                    title_element = article.find(['h1', 'h2', 'h3', 'div'], class_=re.compile('title|headline|article-title|heading', re.I))\n",
        "                    title_text = title_element.text.strip() if title_element else \"No Title Found\"\n",
        "                    article_links_to_visit.append({'title': title_text, 'url': article_url})\n",
        "                    scraped_urls.add(article_url)\n",
        "            if article_links_to_visit:\n",
        "                break\n",
        "\n",
        "        # Check for next page link\n",
        "        next_page = soup.find('a', class_=re.compile('next|pagination-next|load-more', re.I))\n",
        "        if next_page and 'href' in next_page.attrs:\n",
        "            next_url = 'https://trt.global' + next_page['href'] if next_page['href'].startswith('/') else next_page['href']\n",
        "            driver.get(next_url)\n",
        "            time.sleep(3)\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    # Scrape Individual Articles with Retry\n",
        "    max_retries = 3\n",
        "    for i, article_info in enumerate(article_links_to_visit):\n",
        "        article_url = article_info['url']\n",
        "        initial_title = article_info['title']\n",
        "        retries = 0\n",
        "        while retries < max_retries:\n",
        "            try:\n",
        "                driver.get(article_url)\n",
        "                WebDriverWait(driver, 15).until(  # Increased wait time to 15 seconds\n",
        "                    EC.presence_of_element_located((By.CSS_SELECTOR, 'div[class*=\"article-body\"], div[class*=\"content\"], article, main'))\n",
        "                )\n",
        "                article_soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "\n",
        "                # Extract title with fallback\n",
        "                title_element = article_soup.find(['h1', 'h2', 'h3', 'div'], class_=re.compile('title|headline|article-title|heading', re.I))\n",
        "                full_title = title_element.text.strip() if title_element else initial_title\n",
        "\n",
        "                # Extract date with fallback\n",
        "                date_element = article_soup.find(['time', 'p', 'span', 'div'], class_=re.compile('date|time|published|post-date', re.I))\n",
        "                date = date_element.text.strip() if date_element else \"Date not found\"\n",
        "\n",
        "                # Extract section with fallback\n",
        "                section_element = article_soup.find('a', class_=re.compile('category|section', re.I)) or article_soup.find('div', class_=re.compile('category|section', re.I))\n",
        "                section = section_element.text.strip() if section_element and section_element.text.strip() else \"Politics\"\n",
        "\n",
        "                # Extract uri\n",
        "                uri = article_url\n",
        "\n",
        "                # Extract content in paragraph format\n",
        "                body_selectors = ['div[class*=\"article-body\"]', 'div[class*=\"content\"]', 'article', 'main']\n",
        "                article_content = \"\"\n",
        "                for sel in body_selectors:\n",
        "                    article_body = article_soup.select_one(sel)\n",
        "                    if article_body:\n",
        "                        paragraphs = article_body.find_all(['p', 'div'], recursive=False)\n",
        "                        article_content = \"\\n\\n\".join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))\n",
        "                        break\n",
        "                if not article_content:\n",
        "                    article_content = \"Content not found.\"\n",
        "\n",
        "                all_articles_data.append({\n",
        "                    'date': date,\n",
        "                    'section': section,\n",
        "                    'uri': uri,\n",
        "                    'title': full_title,\n",
        "                    'description': article_content\n",
        "                })\n",
        "                print(f\"Successfully scraped: '{full_title}' from {article_url}\")\n",
        "                break\n",
        "            except (TimeoutException, NoSuchElementException, WebDriverException) as e:\n",
        "                retries += 1\n",
        "                print(f\"Error scraping {article_url} (Attempt {retries}/{max_retries}): {str(e)}\")\n",
        "                if retries == max_retries:\n",
        "                    print(f\"Failed to scrape {article_url} after {max_retries} attempts\")\n",
        "                time.sleep(2 ** retries)  # Exponential backoff\n",
        "            except Exception as e:\n",
        "                print(f\"Unexpected error scraping {article_url}: {str(e)}\")\n",
        "                break\n",
        "        time.sleep(1)\n",
        "\n",
        "    # Save to CSV\n",
        "    if all_articles_data:\n",
        "        csv_filename = 'trt_politics_news.csv'\n",
        "        keys = ['date', 'section', 'uri', 'title', 'description']\n",
        "        with open(csv_filename, 'w', newline='', encoding='utf-8') as output_file:\n",
        "            dict_writer = csv.DictWriter(output_file, fieldnames=keys)\n",
        "            dict_writer.writeheader()\n",
        "            dict_writer.writerows(all_articles_data)\n",
        "        print(f\"Saved {len(all_articles_data)} articles to {csv_filename}\")\n",
        "    else:\n",
        "        print(\"No articles scraped.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Critical error: {e}\")\n",
        "finally:\n",
        "    if driver:\n",
        "        driver.quit()\n",
        "\n",
        "if platform.system() == \"Emscripten\":\n",
        "    pass\n",
        "else:\n",
        "    if __name__ == \"__main__\":\n",
        "        pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fKmEMdJz9PU",
        "outputId": "cca1757a-6797-4c0e-f89b-c30053fbb2ac"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing Python packages...\n",
            "Python packages installed.\n",
            "Setting up Google Chrome stable browser...\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Warning: apt-key is deprecated. Manage keyring files in trusted.gpg.d instead (see apt-key(8)).\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Google Chrome stable browser installed.\n",
            "Successfully scraped: 'No Title Found' from https://trt.global/world/article/5bc24073797c\n",
            "Successfully scraped: 'No Title Found' from https://trt.global/world/article/114c2add2514\n",
            "Successfully scraped: 'No Title Found' from https://trt.global/world/article/e980eed3e9d1\n",
            "Successfully scraped: 'No Title Found' from https://trt.global/world/article/3263d7078994\n",
            "Successfully scraped: 'No Title Found' from https://trt.global/world/article/f47ccb6ccf14\n",
            "Successfully scraped: 'No Title Found' from https://trt.global/world/article/105beb35615c\n",
            "Successfully scraped: 'No Title Found' from https://trt.global/world/article/0e103601e717\n",
            "Successfully scraped: 'No Title Found' from https://trt.global/world/article/036a8fa67dfc\n",
            "Successfully scraped: 'No Title Found' from https://trt.global/world/article/d7e91cd73022\n",
            "Successfully scraped: 'No Title Found' from https://trt.global/world/article/5081f8ade0cf\n",
            "Successfully scraped: 'No Title Found' from https://trt.global/world/article/743d1f99e054\n",
            "Successfully scraped: 'No Title Found' from https://trt.global/world/article/5ca4da6ba352\n",
            "Successfully scraped: 'No Title Found' from https://trt.global/world/article/41081fcad19a\n",
            "Successfully scraped: 'No Title Found' from https://trt.global/world/article/bdab1674da14\n",
            "Successfully scraped: 'No Title Found' from https://trt.global/world/article/c463bb603a61\n",
            "Successfully scraped: 'No Title Found' from https://trt.global/world/article/e9dd5610ec7b\n",
            "Successfully scraped: 'No Title Found' from https://trt.global/world/article/6a67899a84a9\n",
            "Successfully scraped: 'No Title Found' from https://trt.global/world/author/675af7f56c539f4300fd620e\n",
            "Saved 18 articles to trt_politics_news.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8ZlAYcJm1Mxq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}